{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-Y7FDB15luO",
        "outputId": "5bdb9894-f6b0-45a9-a0d8-bc701f3c5eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.320-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.43 (from langchain)\n",
            "  Downloading langsmith-0.0.49-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.320 langsmith-0.0.49 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting chromadb==0.4.3\n",
            "  Downloading chromadb-0.4.3-py3-none-any.whl (399 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.0/399.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.3) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.3) (2.31.0)\n",
            "Requirement already satisfied: pydantic<2.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.3) (1.10.13)\n",
            "Collecting chroma-hnswlib==0.7.1 (from chromadb==0.4.3)\n",
            "  Downloading chroma-hnswlib-0.7.1.tar.gz (30 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi<0.100.0,>=0.95.2 (from chromadb==0.4.3)\n",
            "  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb==0.4.3)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.3) (1.23.5)\n",
            "Collecting posthog>=2.4.0 (from chromadb==0.4.3)\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.3) (4.5.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb==0.4.3)\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb==0.4.3)\n",
            "  Downloading onnxruntime-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m122.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from chromadb==0.4.3)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypika>=0.48.9 (from chromadb==0.4.3)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.3) (4.66.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb==0.4.3)\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.3) (6.1.0)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.3)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.3)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.3) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.3) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.3) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.3) (1.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb==0.4.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb==0.4.3) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb==0.4.3) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.4.3)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==0.4.3)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb==0.4.3) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb==0.4.3) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb==0.4.3) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb==0.4.3) (2.0.7)\n",
            "Collecting huggingface_hub<0.18,>=0.16.4 (from tokenizers>=0.13.2->chromadb==0.4.3)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.3) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb==0.4.3)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb==0.4.3)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.3)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.3) (6.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb==0.4.3)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m126.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.3)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb==0.4.3)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.3) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.3) (2023.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.3) (3.7.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.3)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.3) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.3) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.3) (1.1.3)\n",
            "Building wheels for collected packages: chroma-hnswlib, pypika\n",
            "  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.1-cp310-cp310-linux_x86_64.whl size=2272358 sha256=af657715666d5124076c22f022d4d1d7a31b47e585ac5b3caf3cf8716f18f064\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/f2/d2/3f32228e9f4713a9f32a468de8bbc3c642f7805ebef888418b\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=10a30c8804bf7195198bebea384d06b2eb4b83f4a2fefbc1742dc7fdab2c6a58\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built chroma-hnswlib pypika\n",
            "Installing collected packages: pypika, monotonic, websockets, uvloop, python-dotenv, pulsar-client, overrides, humanfriendly, httptools, h11, chroma-hnswlib, backoff, watchfiles, uvicorn, starlette, posthog, huggingface_hub, coloredlogs, tokenizers, onnxruntime, fastapi, chromadb\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 chroma-hnswlib-0.7.1 chromadb-0.4.3 coloredlogs-15.0.1 fastapi-0.99.1 h11-0.14.0 httptools-0.6.1 huggingface_hub-0.17.3 humanfriendly-10.0 monotonic-1.6 onnxruntime-1.16.1 overrides-7.4.0 posthog-3.0.2 pulsar-client-3.3.0 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 tokenizers-0.14.1 uvicorn-0.23.2 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.1\n",
            "Requirement already satisfied: langchain[docarray] in /usr/local/lib/python3.10/dist-packages (0.0.320)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (0.0.49)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (8.2.3)\n",
            "Collecting docarray[hnswlib]<0.33.0,>=0.32.0 (from langchain[docarray])\n",
            "  Downloading docarray-0.32.1-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain[docarray]) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain[docarray]) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain[docarray]) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain[docarray]) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain[docarray]) (0.9.0)\n",
            "Collecting orjson>=3.8.2 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray])\n",
            "  Downloading orjson-3.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.10/dist-packages (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (13.6.0)\n",
            "Collecting types-requests>=2.28.11.6 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray])\n",
            "  Downloading types_requests-2.31.0.10-py3-none-any.whl (14 kB)\n",
            "Collecting hnswlib>=0.6.2 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray])\n",
            "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (3.20.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain[docarray]) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain[docarray]) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain[docarray]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain[docarray]) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain[docarray]) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain[docarray]) (23.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (2.16.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain[docarray]) (1.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (0.1.2)\n",
            "Building wheels for collected packages: hnswlib\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl size=2207717 sha256=d4c37c66f08349a316feba5b267f2cc9cb784ebf31d8f512122f601967fe0141\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/ae/ec/235a682e0041fbaeee389843670581ec6c66872db856dfa9a4\n",
            "Successfully built hnswlib\n",
            "Installing collected packages: types-requests, orjson, hnswlib, docarray\n",
            "Successfully installed docarray-0.32.1 hnswlib-0.7.0 orjson-3.9.9 types-requests-2.31.0.10\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.16.4-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.6/276.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.16.4\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.50.2-py3-none-any.whl (20.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.99.1)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.6.1 (from gradio)\n",
            "  Downloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.17.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.9.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.13)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.5.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.2)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.6.1->gradio) (2023.6.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2023.7.22)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.27.0)\n",
            "Collecting httpcore<0.19.0,>=0.18.0 (from httpx->gradio)\n",
            "  Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.19.0,>=0.18.0->httpx->gradio) (3.7.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.10.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.19.0,>=0.18.0->httpx->gradio) (1.1.3)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=99be8da2cfb85e7a3b9e7d018f38341ad012bd765d8b2fb6b5874cab34a18882\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, semantic-version, python-multipart, aiofiles, httpcore, httpx, gradio-client, gradio\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 12.0\n",
            "    Uninstalling websockets-12.0:\n",
            "      Successfully uninstalled websockets-12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 ffmpy-0.3.1 gradio-3.50.2 gradio-client-0.6.1 httpcore-0.18.0 httpx-0.25.0 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "# Install all required packages\n",
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install chromadb==0.4.3\n",
        "!pip install tiktoken\n",
        "!pip install \"langchain[docarray]\"\n",
        "!pip install pypdf\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "ONfwRttb5qLy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "import IPython.display\n",
        "from PIL import Image\n",
        "import base64\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import openai\n",
        "import tkinter as tk\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.document_loaders import (\n",
        "    DataFrameLoader,\n",
        "    TextLoader,\n",
        "    PyPDFLoader\n",
        ")\n",
        "from langchain.text_splitter import (\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    CharacterTextSplitter\n",
        ")\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import (\n",
        "    DocArrayInMemorySearch,\n",
        "    Chroma\n",
        ")\n",
        "from langchain.chains import (\n",
        "    RetrievalQA,\n",
        "    ConversationalRetrievalChain\n",
        ")\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from dotenv import load_dotenv, find_dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "aEIcHJjcV-Kn"
      },
      "outputs": [],
      "source": [
        "key = \"sk-xxxxxxxxx\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nhlAMgHOoinN"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ovu7wunprB64"
      },
      "outputs": [],
      "source": [
        "# load data and preprocess it\n",
        "def squad_json_to_dataframe(file_path, record_path=['data','paragraphs','qas','answers']):\n",
        "    \"\"\"\n",
        "    input_file_path: path to the squad json file.\n",
        "    record_path: path to deepest level in json file default value is\n",
        "    ['data','paragraphs','qas','answers']\n",
        "    \"\"\"\n",
        "\n",
        "    file = json.loads(open(file_path).read())\n",
        "    # parsing different level's in the json file\n",
        "    js = pd.json_normalize(file, record_path)\n",
        "    m = pd.json_normalize(file, record_path[:-1])\n",
        "    r = pd.json_normalize(file,record_path[:-2])\n",
        "\n",
        "    # combining it into single dataframe\n",
        "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
        "    m['context'] = idx\n",
        "    data = m[['id','question','context','answers']].set_index('id').reset_index()\n",
        "    data['c_id'] = data['context'].factorize()[0]\n",
        "    return data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCX9S8nrrDq-",
        "outputId": "7f9f06a1-c2d3-492f-ebc0-cf8a1b44b836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set:\n",
            "\n",
            "                             id  \\\n",
            "87594  5735d259012e2f140011a09d   \n",
            "87595  5735d259012e2f140011a09e   \n",
            "87596  5735d259012e2f140011a09f   \n",
            "87597  5735d259012e2f140011a0a0   \n",
            "87598  5735d259012e2f140011a0a1   \n",
            "\n",
            "                                                question  \\\n",
            "87594  In what US state did Kathmandu first establish...   \n",
            "87595               What was Yangon previously known as?   \n",
            "87596  With what Belorussian city does Kathmandu have...   \n",
            "87597  In what year did Kathmandu create its initial ...   \n",
            "87598                      What is KMC an initialism of?   \n",
            "\n",
            "                                                 context  \\\n",
            "87594  Kathmandu Metropolitan City (KMC), in order to...   \n",
            "87595  Kathmandu Metropolitan City (KMC), in order to...   \n",
            "87596  Kathmandu Metropolitan City (KMC), in order to...   \n",
            "87597  Kathmandu Metropolitan City (KMC), in order to...   \n",
            "87598  Kathmandu Metropolitan City (KMC), in order to...   \n",
            "\n",
            "                           answers   c_id  \n",
            "87594                       Oregon  18890  \n",
            "87595                      Rangoon  18890  \n",
            "87596                        Minsk  18890  \n",
            "87597                         1975  18890  \n",
            "87598  Kathmandu Metropolitan City  18890  \n",
            "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            "Test set:\n",
            "\n",
            "                             id  \\\n",
            "10565  5737aafd1c456719005744fb   \n",
            "10566  5737aafd1c456719005744fc   \n",
            "10567  5737aafd1c456719005744fd   \n",
            "10568  5737aafd1c456719005744fe   \n",
            "10569  5737aafd1c456719005744ff   \n",
            "\n",
            "                                                question  \\\n",
            "10565  What is the metric term less used than the New...   \n",
            "10566  What is the kilogram-force sometimes reffered ...   \n",
            "10567  What is a very seldom used unit of mass in the...   \n",
            "10568  What seldom used term of a unit of force equal...   \n",
            "10569  What is the seldom used force unit equal to on...   \n",
            "\n",
            "                                                 context         answers  c_id  \n",
            "10565  The pound-force has a metric counterpart, less...  kilogram-force  2066  \n",
            "10566  The pound-force has a metric counterpart, less...        kilopond  2066  \n",
            "10567  The pound-force has a metric counterpart, less...            slug  2066  \n",
            "10568  The pound-force has a metric counterpart, less...             kip  2066  \n",
            "10569  The pound-force has a metric counterpart, less...          sthène  2066  \n",
            "Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n"
          ]
        }
      ],
      "source": [
        "# load QA dataset\n",
        "filename_train = \"train-v1.1.json\"\n",
        "filename_test = \"dev-v1.1.json\"\n",
        "\n",
        "data_train = squad_json_to_dataframe(filename_train)\n",
        "data_test = squad_json_to_dataframe(filename_test)\n",
        "\n",
        "\n",
        "\n",
        "data_train['answers'] = data_train['answers'].apply(lambda x: x[0]['text'] if x else None)\n",
        "data_test['answers'] = data_test['answers'].apply(lambda x: x[0]['text'] if x else None)\n",
        "\n",
        "print('Train set:\\n')\n",
        "print(data_train.tail())\n",
        "print(data_train['context'][0])\n",
        "\n",
        "print('Test set:\\n')\n",
        "print(data_test.tail())\n",
        "print(data_test['context'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mOysh0gsn8Ro"
      },
      "outputs": [],
      "source": [
        "# create a new data structure combine questions and answers\n",
        "# add $ at then end so its going to be easier to chunking later\n",
        "data_train['qa'] = data_train['question'] +data_train['answers']+'$'\n",
        "data_test['qa'] = data_test['question'] +data_test['answers']+'$'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "16EUmMPNqdpz"
      },
      "outputs": [],
      "source": [
        "# load the dataframe into loader\n",
        "# context\n",
        "loader_train = DataFrameLoader(data_train, page_content_column=\"qa\")\n",
        "doc_train = loader_train.load()\n",
        "\n",
        "\n",
        "loader_test = DataFrameLoader(data_test, page_content_column=\"qa\")\n",
        "doc_test = loader_test.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkHJYmpvWzIl",
        "outputId": "be89fb9e-bb07-421b-e972-d7b843968688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?Saint Bernadette Soubirous$' metadata={'id': '5733be284776f41900661182', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'answers': 'Saint Bernadette Soubirous', 'c_id': 0}\n",
            "page_content='Which NFL team represented the AFC at Super Bowl 50?Denver Broncos$' metadata={'id': '56be4db0acb8001400a502ec', 'question': 'Which NFL team represented the AFC at Super Bowl 50?', 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'answers': 'Denver Broncos', 'c_id': 0}\n"
          ]
        }
      ],
      "source": [
        "print(doc_train[0])\n",
        "print(doc_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3380oQZwsTgi"
      },
      "outputs": [],
      "source": [
        "# get 1/3 of data\n",
        "doc_train = doc_train[:1000]\n",
        "doc_test = doc_test[0:1000]\n",
        "\n",
        "#print(len(doc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OZfI4mvKses1"
      },
      "outputs": [],
      "source": [
        "# splitting text into the specific chunck sizes\n",
        "# defining the overlap size for each chunck\n",
        "\n",
        "\n",
        "#from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"$\",\n",
        "    chunk_size = 125,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len,\n",
        "    is_separator_regex = False,\n",
        ")\n",
        "\n",
        "#text_splitter = RecursiveCharacterTextSplitter(\n",
        "#    chunk_size = 50,\n",
        "#    chunk_overlap = 10\n",
        "#)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "12ZVT8fRwAfi"
      },
      "outputs": [],
      "source": [
        "# split our documnet data\n",
        "splits_train = text_splitter.split_documents(doc_train)\n",
        "splits_test = text_splitter.split_documents(doc_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5MK2e7FwFrR",
        "outputId": "c4bc0990-684d-4e85-ee2f-ec5fadf55849"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1001"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(splits_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qGGyjFfNwN_t"
      },
      "outputs": [],
      "source": [
        "# load the embedding model\n",
        "embedding = OpenAIEmbeddings(request_timeout=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igxm_IIqxsG_",
        "outputId": "c54ccc57-471e-4758-aff9-b07e3ab46dfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-3.5-turbo\n"
          ]
        }
      ],
      "source": [
        "# get the specific gpt model\n",
        "current_date = datetime.datetime.now().date()\n",
        "if current_date < datetime.date(2023, 9, 2):\n",
        "    llm_name = \"gpt-3.5-turbo-0301\"\n",
        "else:\n",
        "    llm_name = \"gpt-3.5-turbo\"\n",
        "print(llm_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "9Ctt48muNecl"
      },
      "outputs": [],
      "source": [
        "# create vector database for train and test sets  from data\n",
        "#db_train = DocArrayInMemorySearch.from_documents(doc_train, embedding)\n",
        "\n",
        "#db_test = DocArrayInMemorySearch.from_documents(doc_test, embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTOEIk1MOTvy",
        "outputId": "80d305bb-ce4f-4127-d0c6-16d15fe62b24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question is:  How many solo tackles did Von Miller make at Super Bowl 50?\n",
            "answers is:  five solo tackles\n",
            "########\n",
            "\n",
            "\n",
            "question is:  Who did the Panthers beat in the divisional round?\n",
            "answers is:  Seattle Seahawks\n",
            "########\n",
            "\n",
            "\n",
            "question is:  Who recovered the strip ball?\n",
            "answers is:  wards\n",
            "########\n",
            "\n",
            "\n",
            "question is:  Who received the pass that was ruled incomplete and confirmed after a challenge?\n",
            "answers is:  Jerricho Cotchery\n",
            "########\n",
            "\n",
            "\n",
            "question is:  Aside from BBC Radio 5, what radio station will broadcast the game?\n",
            "answers is:  5 Live Sports Extra\n",
            "########\n",
            "\n",
            "\n",
            "question is:  What team did Kubiak play for in Super Bowl XXI?\n",
            "answers is:  Broncos\n",
            "########\n",
            "\n",
            "\n",
            "question is:  What did the NFL do to the playing field at Levi's Stadium before the Super Bowl?\n",
            "answers is:  a new playing surface\n",
            "########\n",
            "\n",
            "\n",
            "question is:  What is being described when simplicity of geometrical forms are teamed with inspiration from the Roman period?\n",
            "answers is:  neoclassical architecture\n",
            "########\n",
            "\n",
            "\n",
            "question is:  When were the two finalists for hosting Super Bowl 50 announced?\n",
            "answers is:  October 16, 2012\n",
            "########\n",
            "\n",
            "\n",
            "question is:  How many touchdowns did Newton have in 2015?\n",
            "answers is:  45\n",
            "########\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# print tests\n",
        "i = 10\n",
        "for j in range(i):\n",
        "  random_index = random.randint(0, 1000)\n",
        "  print('question is: ', data_test['question'][random_index])\n",
        "  print('answers is: ', data_test['answers'][random_index])\n",
        "  print('########\\n\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LrssyUdTf5sO"
      },
      "outputs": [],
      "source": [
        "#vectordb_train = db_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UEowBWcLwhbv"
      },
      "outputs": [],
      "source": [
        "# vector store database\n",
        "vectordb_train = Chroma.from_documents(\n",
        "    documents=splits_train,\n",
        "    embedding=embedding,\n",
        ")\n",
        "\n",
        "#vectordb_test = Chroma.from_documents(\n",
        "#    documents=splits_test,\n",
        "#    embedding=embedding,\n",
        "#)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LeLBSHjHgWvs"
      },
      "outputs": [],
      "source": [
        "# vector store database\n",
        "#vectordb_train = Chroma.from_documents(\n",
        "#    persist_directory='test',\n",
        "#    documents=splits_train,\n",
        "#    embedding=embedding,\n",
        "#)\n",
        "\n",
        "\n",
        "#vectordb_test.squad_json_to_dataframe(filename_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaUTU3PjnzcJ",
        "outputId": "e961fede-1382-4ed0-eaa9-1807e1a6682f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Navy Blue and Gold Rush\n"
          ]
        }
      ],
      "source": [
        "# check similarity\n",
        "\n",
        "question = \"What are major topics for this class?\"\n",
        "question = \"What color were the Bronco's uniforms in Super Bowl 50?\"\n",
        "docs = vectordb_train.similarity_search(question,k=1)\n",
        "print(docs[0].metadata['answers'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VB6_Pun3ncyg",
        "outputId": "ddd068c6-a2ab-4ed7-c65f-dab94df12173"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Denver Broncos played against the Atlanta Falcons in Super Bowl XXXIII.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# create chatbot\n",
        "llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
        "llm.predict(\"Hello world!\")\n",
        "llm.predict(\"What is the Grotto at Notre Dame?\")\n",
        "llm.predict(\"What team did the Denver Broncos play in  Super Bowl XXXIII?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ptSpyzbb_DLB"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "kmx7ymWo5r97"
      },
      "outputs": [],
      "source": [
        "# Build prompt\n",
        "template = \"\"\"\n",
        "You are like an QA agent that you suppose to answer the question that you know.\\n\n",
        "You will always gretting every one at the beging, also you can ask for their name so you will respond back with their name to be more polit.\\n\n",
        "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.\\n\n",
        "Also, if you answered any question except being greedy you can say something like \"Do you have any other question that I can help with?\".\\n\n",
        "If the person says I don't have any furthur questions, just say something like 'I am always here to help you with question'.\n",
        "{context}\\n\n",
        "\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n",
        "\n",
        "# Run chain\n",
        "retriever = vectordb_train.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":8})\n",
        "#retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .5})\n",
        "qa_chain = RetrievalQA.from_chain_type(llm,\n",
        "                                       #retriever=vectordb_train.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .1}),\n",
        "                                       retriever=retriever,\n",
        "                                       return_source_documents=True,\n",
        "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gCrmB7FB1ZT4",
        "outputId": "15e38f20-5db8-4ff8-a8dc-a38bd38d1dbf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't know.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "# test topics\n",
        "question = \"Is probability a class topic?\"\n",
        "question = \"What color were the Bronco's uniforms in Super Bowl 50?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "eMo9m-Vk-e-s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auV2o1nOePVt",
        "outputId": "62da62a8-f4ea-48b9-ba54-56a0df7502d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question is:  Which NFL team represented the AFC at Super Bowl 50?\n",
            "answers is:  Denver Broncos\n",
            "########\n",
            "\n",
            "\n",
            "question is:  Which NFL team represented the NFC at Super Bowl 50?\n",
            "answers is:  Carolina Panthers\n",
            "########\n",
            "\n",
            "\n",
            "question is:  Where did Super Bowl 50 take place?\n",
            "answers is:  Santa Clara, California\n",
            "########\n",
            "\n",
            "\n",
            "question is:  Which NFL team won Super Bowl 50?\n",
            "answers is:  Denver Broncos\n",
            "########\n",
            "\n",
            "\n",
            "question is:  What color was used to emphasize the 50th anniversary of the Super Bowl?\n",
            "answers is:  gold\n",
            "########\n",
            "\n",
            "\n",
            "question is:  What was the theme of Super Bowl 50?\n",
            "answers is:  \"golden anniversary\"\n",
            "########\n",
            "\n",
            "\n",
            "question is:  What day was the game played on?\n",
            "answers is:  February 7, 2016\n",
            "########\n",
            "\n",
            "\n",
            "question is:  What is the AFC short for?\n",
            "answers is:  American Football Conference\n",
            "########\n",
            "\n",
            "\n",
            "question is:  What was the theme of Super Bowl 50?\n",
            "answers is:  \"golden anniversary\"\n",
            "########\n",
            "\n",
            "\n",
            "question is:  What does AFC stand for?\n",
            "answers is:  American Football Conference\n",
            "########\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# print tests\n",
        "i = 10\n",
        "for j in range(i):\n",
        "  print('question is: ', data_test['question'][j])\n",
        "  print('answers is: ', data_test['answers'][j])\n",
        "  print('########\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "nVHQdJE0zbWh"
      },
      "outputs": [],
      "source": [
        "# define memory\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "D-FDy3a79WeI"
      },
      "outputs": [],
      "source": [
        "#vectordb.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .5})\n",
        "retriever=vectordb_train.as_retriever()\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    retriever=vectordb_train.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .5}),\n",
        "    memory=memory,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "6QVR2Y1FzfmQ"
      },
      "outputs": [],
      "source": [
        "question = \" How many student news papers are found at Notre Dame?\"\n",
        "question = \" What was the theme of Super Bowl 50?\"\n",
        "result = qa({\"question\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "K6DMSs0rzgF1",
        "outputId": "a749d003-0421-4508-a353-14fce924391a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The theme of Super Bowl 50 was not specified.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "result['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "cKvsOx-AvhzQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "question = \"What is the daily student paper at Notre Dame called?\"\n",
        "result = qa({\"question\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SUo3a42Qvhxd",
        "outputId": "6267e90a-6f49-420d-e0f0-8913a8960c54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The daily student paper at Notre Dame is called The Observer.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "result['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "w_4U-cEtvhu6"
      },
      "outputs": [],
      "source": [
        "# chatbot model initialization\n",
        "# Build prompt\n",
        "template = \"\"\"\n",
        "You are like an QA agent that you suppose to answer the question that you know.\\n\n",
        "You will always gretting every one at the beging, also you can ask for their name so you will respond back with their name to be more polit.\\n\n",
        "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.\\n\n",
        "Also, if you answered any question except being greedy you can say something like \"Do you have any other question that I can help with?\".\\n\n",
        "If the person says I don't have any furthur questions, just say something like 'I am always here to help you with question'.\n",
        "{context}\\n\n",
        "\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n",
        "#vectordb.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .5})\n",
        "retriever=vectordb_train.as_retriever()\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    retriever=vectordb_train.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .2}),\n",
        "    memory=memory,\n",
        "    combine_docs_chain_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "0a8PdqcCBVEa"
      },
      "outputs": [],
      "source": [
        "\n",
        "question = \"What color were the Bronco's uniforms in Super Bowl 50?\"\n",
        "result = qa({\"question\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTxhojh9CjbG",
        "outputId": "51a5aee9-e9f4-4447-a036-cf05e4d59ad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The uniforms of the Broncos in Super Bowl 50 were orange. Do you have any other question that I can help with?\n"
          ]
        }
      ],
      "source": [
        "print(result['answer'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for t in data_test['question']:\n",
        "  i += 1\n",
        "  if i ==10:\n",
        "    break\n",
        "\n",
        "  print(t)\n",
        "print(len(data_test['question']))\n",
        "\n",
        "#data_test = data_test[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9xRDsNZyggX",
        "outputId": "751b97c1-c27d-4276-daf7-794b3590ca5e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Which NFL team represented the AFC at Super Bowl 50?\n",
            "Which NFL team represented the NFC at Super Bowl 50?\n",
            "Where did Super Bowl 50 take place?\n",
            "Which NFL team won Super Bowl 50?\n",
            "What color was used to emphasize the 50th anniversary of the Super Bowl?\n",
            "What was the theme of Super Bowl 50?\n",
            "What day was the game played on?\n",
            "What is the AFC short for?\n",
            "What was the theme of Super Bowl 50?\n",
            "10570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.evaluation import load_evaluator\n",
        "\n",
        "evaluator = load_evaluator(\"qa\")\n",
        "evaluator.evaluate_strings(\n",
        "    prediction=\"The theme of Super Bowl 50 was 'The Golden Super Bowl.' Do you have any other questions that I can help with?\",\n",
        "    input=\" What was the theme of Super Bowl 50?\",\n",
        "    reference=\"golden anniversary\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C2rRtOd9J7q",
        "outputId": "0d5bdb24-4a47-4bcb-857e-a292254a022e"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ugb_5Rr17KMD",
        "outputId": "56e273d5-238b-4b53-cdba-a9936dab150e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define evaluation function\n",
        "\n",
        "container_list = []\n",
        "#container = {}\n",
        "for i in range(50):\n",
        "  container = {}  # Create a new dictionary in each iteration\n",
        "  #print(data_train['question'][i])\n",
        "  predict = qa({\"question\": data_train['question'][i]})\n",
        "  #print(88888)\n",
        "  container['prediction'] = predict['answer'].split('Do you have any other question that I can help with?')[0]\n",
        "  container['input'] = data_train['question'][i]\n",
        "  container['reference'] = data_train['answers'][i]\n",
        "  #print(container)\n",
        "  container_list.append(container)\n",
        "  #print(container_list)\n",
        "\n",
        "  #print(f'prediction:' ,container['prediction'])\n",
        "  #print(f'input:', container['input'])\n",
        "  #print(f'reference:' ,container['reference'])\n",
        "\n",
        "  #print('########\\n')\n",
        "\n",
        "#print(container_list)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wg6bV2dh9rMV"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## check the lenght\n",
        "#container_list[0]['prediction']\n",
        "#print(len(container_list))"
      ],
      "metadata": {
        "id": "MBJdI5Yt_FJQ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## print statement to print samples\n",
        "#for i in range(len(container_list)):\n",
        "\n",
        "      #prediction=container_list[i]['prediction']\n",
        "      #input=container_list[i]['input']\n",
        "      #reference=container_list[i]['reference']\n",
        "\n",
        "      #print(f'prediction: {prediction}')\n",
        "      #print(f'input: {input}')\n",
        "      #print(f'reference: {reference}')\n",
        "\n",
        "      #print('########\\n')\n"
      ],
      "metadata": {
        "id": "z9zEqycPAjuo"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the eval list that includes: {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}\n",
        "\n",
        "eval_list = []\n",
        "for i in range(len(container_list)):\n",
        "  eval_results = evaluator.evaluate_strings(\n",
        "      prediction=container_list[i]['prediction'],\n",
        "      input=container_list[i]['input'],\n",
        "      reference=container_list[i]['reference'],\n",
        "  )\n",
        "  eval_list.append(eval_results)\n"
      ],
      "metadata": {
        "id": "mOa86jCu9rJU"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(eval_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pc2oXKWT9rHc",
        "outputId": "514313ba-637e-441d-f977-e2ec8acb95ff"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'INCORRECT', 'value': 'INCORRECT', 'score': 0}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}, {'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total, correct, idx=0,0,0\n",
        "for v in eval_list: # loop to compute accuracy\n",
        "\n",
        "    if v['reasoning'].strip()=='CORRECT':\n",
        "        correct+=1\n",
        "    else: # record index for incorrect predictions\n",
        "        print(f\"query: {container_list[idx]['input']}\")\n",
        "        print(f\"Answer: {container_list[idx]['reference']}\")\n",
        "        print(f\"Prediction: {container_list[idx]['prediction']}\")\n",
        "    total+=1; idx+=1\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtZAjNwA9rE9",
        "outputId": "532f50b1-8449-4773-8a78-fe0df181c26a"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query: To whom was John B. Kroc married?\n",
            "Answer: Ray Kroc\n",
            "Prediction: John B. Kroc was married to Ray Kroc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'the model prediction accuracy is {correct/total*100} percent')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LNhM0dTuC4p",
        "outputId": "bc1c7118-6f0d-4091-e77c-911a3aac0d59"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the model prediction accuracy is 98.0 percent\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}